{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a50ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\patri\\anaconda3\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\patri\\anaconda3\\lib\\site-packages (1.20.1)\n",
      "Requirement already satisfied: kafka-python in c:\\users\\patri\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\patri\\anaconda3\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\patri\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\patri\\anaconda3\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\patri\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\patri\\anaconda3\\lib\\site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\patri\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\patri\\anaconda3\\lib\\site-packages (from scikit-learn) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy kafka-python scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1af8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kafka import KafkaConsumer  # For data ingestion (requires kafka-python library)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sqlite3  # For local storage simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d29076a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Ingested:\n",
      "   timestamp  sensor_reading\n",
      "0 2025-01-01              69\n",
      "1 2025-01-02              80\n",
      "2 2025-01-03              68\n",
      "3 2025-01-04              79\n",
      "4 2025-01-05              87\n",
      "5 2025-01-06              73\n",
      "6 2025-01-07              51\n",
      "7 2025-01-08              92\n",
      "8 2025-01-09              60\n",
      "9 2025-01-10              60\n",
      "\n",
      "Data Processed (Normalized):\n",
      "   timestamp  sensor_reading  normalized_reading\n",
      "0 2025-01-01              69            0.439024\n",
      "1 2025-01-02              80            0.707317\n",
      "2 2025-01-03              68            0.414634\n",
      "3 2025-01-04              79            0.682927\n",
      "4 2025-01-05              87            0.878049\n",
      "5 2025-01-06              73            0.536585\n",
      "6 2025-01-07              51            0.000000\n",
      "7 2025-01-08              92            1.000000\n",
      "8 2025-01-09              60            0.219512\n",
      "9 2025-01-10              60            0.219512\n",
      "\n",
      "Data Stored in SQLite Database.\n",
      "\n",
      "AI Model Predictions (Next 5 Days):\n",
      "Date: 2025-01-10 00:00:00, Predicted Reading: 66.47\n",
      "Date: 2025-01-11 00:00:00, Predicted Reading: 65.27\n",
      "Date: 2025-01-12 00:00:00, Predicted Reading: 64.06\n",
      "Date: 2025-01-13 00:00:00, Predicted Reading: 62.85\n",
      "Date: 2025-01-14 00:00:00, Predicted Reading: 61.65\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Ingestion\n",
    "def ingest_data():\n",
    "    \"\"\"\n",
    "    Simulates data ingestion from a Kafka topic. \n",
    "    Returns a mock dataframe for demonstration or real data if Kafka is configured.\n",
    "    \"\"\"\n",
    "    # Mock data for testing without Kafka\n",
    "    data = {\n",
    "        \"timestamp\": pd.date_range(start=\"2025-01-01\", periods=10, freq=\"D\"),\n",
    "        \"sensor_reading\": np.random.randint(50, 100, size=10),\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Data Ingested:\")\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "    # Uncomment the following to use Kafka for real data ingestion\n",
    "    # consumer = KafkaConsumer('your_topic', \n",
    "    #                          bootstrap_servers=['localhost:9092'],\n",
    "    #                          value_deserializer=lambda m: json.loads(m.decode('ascii')))\n",
    "    # \n",
    "    # data_list = []\n",
    "    # for message in consumer:\n",
    "    #     data_list.append(message.value)\n",
    "    # \n",
    "    # if not data_list:\n",
    "    #     raise ValueError(\"No data received from Kafka\")\n",
    "    # \n",
    "    # df = pd.DataFrame(data_list)\n",
    "    # consumer.close()\n",
    "    # return df\n",
    "\n",
    "# 2. Data Processing (Simple ETL)\n",
    "def process_data(df):\n",
    "    \"\"\"\n",
    "    Simulates processing by normalizing the sensor readings.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame is empty, cannot process data.\")\n",
    "    \n",
    "    df[\"normalized_reading\"] = (df[\"sensor_reading\"] - df[\"sensor_reading\"].min()) / (\n",
    "        df[\"sensor_reading\"].max() - df[\"sensor_reading\"].min()\n",
    "    )\n",
    "    print(\"\\nData Processed (Normalized):\")\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "# 3. Data Storage\n",
    "def store_data(df):\n",
    "    \"\"\"\n",
    "    Stores processed data into a local SQLite database.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame is empty, cannot store data.\")\n",
    "    \n",
    "    conn = sqlite3.connect(\"data_pipeline.db\")\n",
    "    df.to_sql(\"sensor_data\", conn, if_exists=\"replace\", index=False)\n",
    "    conn.close()\n",
    "    print(\"\\nData Stored in SQLite Database.\")\n",
    "\n",
    "# 4. AI Model Integration (Simple Predictive Model)\n",
    "def ai_model_prediction(df):\n",
    "    \"\"\"\n",
    "    Uses a linear regression model to predict future sensor readings based on time.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame is empty, cannot make predictions.\")\n",
    "    \n",
    "    # Convert timestamp to ordinal for numeric prediction\n",
    "    df[\"timestamp_ordinal\"] = pd.to_datetime(df[\"timestamp\"]).apply(pd.Timestamp.toordinal)\n",
    "\n",
    "    # Train a simple linear regression model\n",
    "    model = LinearRegression()\n",
    "    X = df[[\"timestamp_ordinal\"]]\n",
    "    y = df[\"sensor_reading\"]\n",
    "    try:\n",
    "        model.fit(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in model fitting: {e}\")\n",
    "        return\n",
    "\n",
    "    # Predict future readings for the next 5 days\n",
    "    future_dates = pd.date_range(start=df[\"timestamp\"].max(), periods=5, freq=\"D\")\n",
    "    future_ordinals = future_dates.map(pd.Timestamp.toordinal)\n",
    "    future_readings = model.predict(future_ordinals.values.reshape(-1, 1))\n",
    "\n",
    "    print(\"\\nAI Model Predictions (Next 5 Days):\")\n",
    "    for date, reading in zip(future_dates, future_readings):\n",
    "        print(f\"Date: {date}, Predicted Reading: {reading:.2f}\")\n",
    "\n",
    "# Main Pipeline Execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Step 1: Ingest data\n",
    "        data = ingest_data()\n",
    "\n",
    "        # Step 2: Process data\n",
    "        processed_data = process_data(data)\n",
    "\n",
    "        # Step 3: Store data\n",
    "        store_data(processed_data)\n",
    "\n",
    "        # Step 4: AI Model Prediction\n",
    "        ai_model_prediction(processed_data)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa4864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
